{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Preprocess a Sample of the Dataset\n",
    "def load_and_preprocess_sample_data(sample_size=1000):\n",
    "    # Load the Natural Questions dataset\n",
    "    dataset = load_dataset(\"natural_questions\", split=\"train[:1%]\")  # Use 1% of the dataset\n",
    "    \n",
    "    # Limit the dataset to a smaller sample size\n",
    "    dataset = dataset.select(range(min(sample_size, len(dataset))))\n",
    "    \n",
    "    # Filter relevant fields: question, document_plaintext, long_answer\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [q.strip() for q in examples['question.text']]\n",
    "        targets = [a.strip() for a in examples['annotations.long_answer[0].plaintext']]\n",
    "        model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "        labels = tokenizer(targets, max_length=512, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Tokenize the dataset\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Model and Tokenizer\n",
    "def load_model_and_tokenizer():\n",
    "    tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "    retriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "    model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\n",
    "    return tokenizer, retriever, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Training Arguments\n",
    "def setup_training_args():\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=100,\n",
    "        save_steps=500,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        fp16=True,  # Enable mixed precision\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"bleu\",\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Evaluation Metrics\n",
    "def compute_metrics(pred):\n",
    "    metric = load_metric(\"sacrebleu\")\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = metric.compute(predictions=[decoded_preds], references=[[l] for l in decoded_labels])\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train the Model\n",
    "def train_model(model, tokenized_datasets, training_args):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets,\n",
    "        eval_dataset=tokenized_datasets,  # Use the same dataset for validation (small-scale project)\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Save the Model\n",
    "def save_model(trainer):\n",
    "    trainer.save_model(\"./rag_model\")\n",
    "    tokenizer.save_pretrained(\"./rag_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Perform Inference\n",
    "def perform_inference(question):\n",
    "    model = RagSequenceForGeneration.from_pretrained(\"./rag_model\", retriever=retriever)\n",
    "    tokenizer = RagTokenizer.from_pretrained(\"./rag_model\")\n",
    "    \n",
    "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(inputs[\"input_ids\"])\n",
    "    answer = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e7a65086ab43d6bf9251f539fa46a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45213e51863c44cba846cfd50b59928c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381702169ffa4ee4809b79041ede96e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/287 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2008d28c93477ab7a9cf92739acd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/193M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537ff90c1d4a4aea96dc553a2524ffd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/185M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dd80130cc0442fadd83616e5725182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/189M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21216cf1db2d4b19b2ec96f95f0b5e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/190M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df742196a35b44a9b58abd999345f59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/196M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851171e755c04ae68c111c9c50683c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/190M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2af0b8db8274798b5b931c4013efec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608c8be602394e48a873de9eba4bbf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/307373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Check GPU availability\n",
    "    print(\"GPU Available:\", torch.cuda.is_available())\n",
    "    \n",
    "    # Step 1: Load and preprocess a sample of the data\n",
    "    sample_size = 1000  # Use 1000 examples for training\n",
    "    tokenized_datasets = load_and_preprocess_sample_data(sample_size)\n",
    "    \n",
    "    # Step 2: Load model and tokenizer\n",
    "    tokenizer, retriever, model = load_model_and_tokenizer()\n",
    "    \n",
    "    # Step 3: Setup training arguments\n",
    "    training_args = setup_training_args()\n",
    "    \n",
    "    # Step 4: Train the model\n",
    "    trainer = train_model(model, tokenized_datasets, training_args)\n",
    "    \n",
    "    # Step 5: Save the trained model\n",
    "    save_model(trainer)\n",
    "    \n",
    "    # Step 6: Perform inference\n",
    "    question = \"What is the capital of France?\"\n",
    "    answer = perform_inference(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaV1_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
